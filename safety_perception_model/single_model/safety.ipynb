{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append(\"/code/LLM-crime\")\n",
    "from generate_features import generate_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:44<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9030, 256)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "from safety_perception_dataset import SafetyPerceptionCLIPDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import neptune\n",
    "sys.path.append(\"/code/LLM-crime/safety_perception_model/single_model\")\n",
    "from my_models import TransformerRegressionModel, FeatureViTClassifier\n",
    "sys.path.append(\"/code/LLM-crime\")\n",
    "from custom_clip_train import CLIPModel, CLIPDataset, build_loaders, make_prediction\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def get_img_feature(paras):\n",
    "    CLIP_model_path = os.path.join(paras['save_model_path'], paras['save_model_name'])\n",
    "    save_paths = paras['variables_save_paths']\n",
    "    if not os.path.exists(save_paths):\n",
    "        os.makedirs(save_paths)\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {paras['device']}\")\n",
    "\n",
    "    img_encoder_paras = torch.load(CLIP_model_path)\n",
    "    img_encoder = CLIPModel(paras)\n",
    "    img_encoder.load_state_dict(img_encoder_paras)\n",
    "    baseline_data = pd.read_csv(paras['dataset_path'])\n",
    "    # print(\"baseline_data: \", len(baseline_data))\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(text_tokenizer)\n",
    "\n",
    "    data_loader = build_loaders(baseline_data, tokenizer, mode=\"valid\", cfg_paras=paras)\n",
    "\n",
    "    img_encoder.to(paras['device'])\n",
    "    img_feature, text_feature = make_prediction(img_encoder, data_loader, cfg_paras=paras) # (datasize, 256)\n",
    "    img_feature = np.array(img_feature)\n",
    "    text_feature = np.array(text_feature)\n",
    "\n",
    "    # Save img_feature to a file\n",
    "    np.save(os.path.join(save_paths, 'img_feature.npy'), img_feature)\n",
    "    np.save(os.path.join(save_paths, 'text_feature.npy'), text_feature)\n",
    "    return img_feature, text_feature\n",
    "\n",
    "paras = {\n",
    "    'debug':False,\n",
    "    # 'dataset_path':\"/data2/cehou/LLM_safety/img_text_data/dataset_baseline_baseline_baseline_baseline_1401.pkl\",\n",
    "    'dataset_path':'/data2/cehou/LLM_safety/img_text_data/baseline/tidyed/dataset_baseline_baseline_baseline_baseline_9030_withlabel.csv',\n",
    "    'save_model_path':\"/data2/cehou/LLM_safety/LLM_models/clip_model/test\",\n",
    "    'save_model_name':\"model_baseline_test.pt\",\n",
    "    'device':torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'batch_size':60,\n",
    "    'num_workers':4,\n",
    "    'head_lr':1e-3,\n",
    "    'image_encoder_lr':1e-4,\n",
    "    'text_encoder_lr':1e-5,\n",
    "    'weight_decay':1e-3,\n",
    "    'img_type':'PlacePulse',\n",
    "    'patience':1,\n",
    "    'factor':0.8,\n",
    "    'epochs':400,\n",
    "    'image_embedding':768,\n",
    "    'text_embedding':768,\n",
    "    'max_length':512,\n",
    "    'size':(112,112),\n",
    "    \n",
    "    # models for image and text\n",
    "    'model_name':'resnet50',\n",
    "    'text_encoder_model':\"distilbert-base-uncased\",\n",
    "    'text_tokenizer': \"distilbert-base-uncased\",\n",
    "    'pretrained':True,\n",
    "    'trainable':True,\n",
    "    \n",
    "    # deep learning model parameters\n",
    "    'temperature':0.07,\n",
    "    'projection_dim':256,\n",
    "    'dropout':0.1,\n",
    "    'early_stopping_threshold':5,\n",
    "    \n",
    "    # safety perception\n",
    "    # 'CLIP_model_path': \"/data2/cehou/LLM_safety/LLM_models/clip_model/test/model_baseline_best.pt\",\n",
    "    'variables_save_paths': f\"/data2/cehou/LLM_safety/middle_variables/test\",\n",
    "    'safety_model_save_path' : f\"/data2/cehou/LLM_safety/LLM_models/safety_perception_model/\",\n",
    "    'placepulse_datapath': \"/data2/cehou/LLM_safety/PlacePulse2.0/image_perception_score.csv\",\n",
    "    'eval_path': \"/data2/cehou/LLM_safety/eval/test/\",\n",
    "    'train_type': 'classification',\n",
    "    'safety_epochs': 200,\n",
    "    'CNN_lr': 1e-2,\n",
    "    \n",
    "    }\n",
    "\n",
    "img_feature,_ = get_img_feature(paras)\n",
    "print(img_feature.shape)\n",
    "data = pd.read_csv(paras['placepulse_datapath'])\n",
    "SVI_namelist = pd.read_csv(paras['dataset_path'])\n",
    "namelist = pd.DataFrame([SVI_namelist.loc[i,'Image_ID'] for i in range(len(SVI_namelist))],columns=['Image_ID'])\n",
    "data = namelist.merge(data[data['Category'] == 'safety'], on='Image_ID')\n",
    "data_nonezero = data[data['label'] != 0]\n",
    "data_nonezero_idx = data[data['label'] != 0].index\n",
    "img_feature_nonezero = img_feature[data_nonezero_idx,:]\n",
    "data_nonezero = data_nonezero.reset_index(drop=True)\n",
    "\n",
    "train_len = int(0.7*len(img_feature_nonezero))\n",
    "train_dataset = SafetyPerceptionCLIPDataset(data[:train_len], img_feature[:train_len], paras)\n",
    "valid_dataset = SafetyPerceptionCLIPDataset(data[train_len:], img_feature[train_len:], paras)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nonezero.loc[data_nonezero[data_nonezero['label'] == -1].index,'label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_ID</th>\n",
       "      <th>Score</th>\n",
       "      <th>Category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50f55e9bfdc9f065f0004d92</td>\n",
       "      <td>5.406606</td>\n",
       "      <td>safety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50f55e9cfdc9f065f0004da9</td>\n",
       "      <td>4.219841</td>\n",
       "      <td>safety</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50f55e9cfdc9f065f0004daa</td>\n",
       "      <td>5.941633</td>\n",
       "      <td>safety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50f55e9cfdc9f065f0004dae</td>\n",
       "      <td>4.258011</td>\n",
       "      <td>safety</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50f55e9dfdc9f065f0004dca</td>\n",
       "      <td>5.567581</td>\n",
       "      <td>safety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9023</th>\n",
       "      <td>50f55e1dfdc9f065f0004ae8</td>\n",
       "      <td>6.001295</td>\n",
       "      <td>safety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9024</th>\n",
       "      <td>50f55e1dfdc9f065f0004ae9</td>\n",
       "      <td>5.622535</td>\n",
       "      <td>safety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9025</th>\n",
       "      <td>50f55e1efdc9f065f0004af6</td>\n",
       "      <td>5.988817</td>\n",
       "      <td>safety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9027</th>\n",
       "      <td>50f55e1efdc9f065f0004b03</td>\n",
       "      <td>6.583531</td>\n",
       "      <td>safety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9028</th>\n",
       "      <td>50f55e33fdc9f065f0004b09</td>\n",
       "      <td>3.881578</td>\n",
       "      <td>safety</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5605 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Image_ID     Score Category  label\n",
       "1     50f55e9bfdc9f065f0004d92  5.406606   safety      1\n",
       "2     50f55e9cfdc9f065f0004da9  4.219841   safety      0\n",
       "3     50f55e9cfdc9f065f0004daa  5.941633   safety      1\n",
       "4     50f55e9cfdc9f065f0004dae  4.258011   safety      0\n",
       "6     50f55e9dfdc9f065f0004dca  5.567581   safety      1\n",
       "...                        ...       ...      ...    ...\n",
       "9023  50f55e1dfdc9f065f0004ae8  6.001295   safety      1\n",
       "9024  50f55e1dfdc9f065f0004ae9  5.622535   safety      1\n",
       "9025  50f55e1efdc9f065f0004af6  5.988817   safety      1\n",
       "9027  50f55e1efdc9f065f0004b03  6.583531   safety      1\n",
       "9028  50f55e33fdc9f065f0004b09  3.881578   safety      0\n",
       "\n",
       "[5605 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nonezero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet50Custom(nn.Module):\n",
    "    def __init__(self, input_dim=256, num_classes=10):\n",
    "        super(ResNet50Custom, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load the pre-trained ResNet50 model\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        # Modify the first convolutional layer to accept 1-channel input\n",
    "        self.resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Modify the fully connected layer to match the input dimension and number of classes\n",
    "        self.resnet50.fc = nn.Linear(self.resnet50.fc.in_features, self.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape the input to match the expected input shape of ResNet50\n",
    "        x = x.view(-1, 1, 16, 16)  # Assuming input_dim=256, reshape to (batch_size, 1, 16, 16)\n",
    "        x = self.resnet50(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_dim = 256\n",
    "num_classes = 10\n",
    "model = ResNet50Custom(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "# Create a dummy input tensor with shape (batch_size, input_dim)\n",
    "dummy_input = torch.randn(8, input_dim)  # batch_size=8\n",
    "output = model(dummy_input)\n",
    "print(output.shape)  # Should print torch.Size([8, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(labels)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/code/LLM-crime/safety_perception_model/single_model/safety_perception_dataset.py:29\u001b[0m, in \u001b[0;36mSafetyPerceptionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 29\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;66;03m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;66;03m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_hooks \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m         )\n\u001b[0;32m--> 476\u001b[0m _, image_height, image_width \u001b[38;5;241m=\u001b[39m \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    478\u001b[0m     size \u001b[38;5;241m=\u001b[39m [size]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:78\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(data_loader):\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 6\n",
    "model_dim = 64\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "output_dim = 6\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "image_size = (300,400)\n",
    "\n",
    "# Generate some dummy data\n",
    "batch_size = 32\n",
    "sequence_length = 10\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = TransformerRegressionModel(input_dim, model_dim, num_heads, num_layers, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_P_and_N(select_id, category, data):\n",
    "    data_test = data[(data['left_id'] == select_id) | (data['right_id'] == select_id)]\n",
    "\n",
    "    data_test_group = data_test.groupby('category')\n",
    "    # Check if the category exists in the grouped data\n",
    "    if category not in data_test_group.groups:\n",
    "        # raise KeyError(f\"Category '{category}' not found in the data.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    test_group = data_test_group.get_group(category)\n",
    "\n",
    "    for i,line in test_group.iterrows():\n",
    "        if line['winner'] == 'left':\n",
    "            test_group.loc[i, \"winner_id\"] = line[\"left_id\"]\n",
    "            test_group.loc[i, \"loser_id\"] = line[\"right_id\"]\n",
    "        elif line['winner'] == 'right':\n",
    "            test_group.loc[i, \"winner_id\"] = line[\"right_id\"]\n",
    "            test_group.loc[i, \"loser_id\"] = line[\"left_id\"]\n",
    "        else:\n",
    "            test_group.loc[i, \"winner_id\"] = \"equal\"\n",
    "            test_group.loc[i, \"loser_id\"] = \"equal\"\n",
    "\n",
    "    winner_id = test_group['winner_id'].tolist()\n",
    "    loser_id = test_group['loser_id'].tolist()\n",
    "\n",
    "    p_i = len(test_group[test_group['winner_id'] == select_id]) \n",
    "    e_i = len(test_group[test_group['winner_id'] == \"equal\"]) \n",
    "    n_i = len(test_group) - p_i - e_i\n",
    "\n",
    "    P_i = p_i / (p_i + e_i + n_i)\n",
    "    N_i = n_i / (p_i + e_i + n_i)\n",
    "    return P_i, N_i, winner_id, loser_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_Q(select_id, category, data):\n",
    "    P_i, N_i, winner_id, loser_id = cal_P_and_N(select_id, category, data)\n",
    "    if (P_i is None) or (N_i is None):\n",
    "        return None\n",
    "\n",
    "    sub_pi_list = []\n",
    "    for i in winner_id:\n",
    "        if (i != \"equal\") and (i != select_id):\n",
    "            sub_P_i, _, _, _ = cal_P_and_N(i, category, data)\n",
    "            sub_pi_list.append(sub_P_i)\n",
    "\n",
    "    sub_ni_list = []\n",
    "    for i in loser_id:\n",
    "        if (i != \"equal\") and (i != select_id):\n",
    "            _, sub_N_i, _, _ = cal_P_and_N(i, category, data)\n",
    "            sub_ni_list.append(sub_N_i)\n",
    "\n",
    "    if len(sub_pi_list) == 0:\n",
    "        sub_P_i_avg = 0\n",
    "    else:\n",
    "        sub_P_i_avg = np.mean(sub_pi_list)\n",
    "        \n",
    "    if len(sub_ni_list) == 0:\n",
    "        sub_n_i_avg = 0\n",
    "    else:\n",
    "        sub_n_i_avg = np.mean(sub_ni_list)\n",
    "    Q = (1 / 3) * (P_i + sub_P_i_avg - sub_n_i_avg + 1)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data_nas/cehou/LLM_safety/PlacePulse2.0/metadata/final_data.csv\"\n",
    "img_path = \"/data_nas/cehou/LLM_safety/PlacePulse2.0/photo_dataset/final_photo_dataset\"\n",
    "data = pd.read_csv(data_path)\n",
    "img_id_ls = [i.split('.')[0] for i in os.listdir(img_path)]\n",
    "category = data['category'].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# def calculate_Q_for_image(img_id):\n",
    "#     results = []\n",
    "#     for c in category:\n",
    "#         Q = cal_Q(img_id, c, data)\n",
    "#         results.append([img_id, c, Q])\n",
    "#     return results\n",
    "# Q_ls = []\n",
    "# with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "#     futures = [executor.submit(calculate_Q_for_image, img_id) for img_id in tqdm(img_id_ls)]\n",
    "#     for future in as_completed(futures):\n",
    "#         Q_ls.extend(future.result())\n",
    "#         if len(Q_ls) % 100 == 0:\n",
    "#             Q_df = pd.DataFrame(Q_ls, columns=['Image_ID', 'Category', 'Q_Value'])\n",
    "#             Q_df.to_csv(\"/data_nas/cehou/LLM_safety/image_perception.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_df = pd.DataFrame(Q_ls, columns=['Image_ID', 'Category', 'Q_Value'])\n",
    "Q_df.to_csv(\"/data_nas/cehou/LLM_safety/image_perception.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 5/110688 [00:19<118:28:04,  3.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# futures = [calculate_Q_for_image(img_id) for img_id in tqdm(img_id_ls)]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_id \u001b[38;5;129;01min\u001b[39;00m tqdm(img_id_ls):\n\u001b[0;32m---> 11\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_Q_for_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     Q_ls\u001b[38;5;241m.\u001b[39mextend(results)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Q_ls) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36mcalculate_Q_for_image\u001b[0;34m(img_id)\u001b[0m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m category:\n\u001b[0;32m----> 4\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[43mcal_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([img_id, c, Q])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mcal_Q\u001b[0;34m(select_id, category, data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcal_Q\u001b[39m(select_id, category, data):\n\u001b[0;32m----> 2\u001b[0m     P_i, N_i, winner_id, loser_id \u001b[38;5;241m=\u001b[39m \u001b[43mcal_P_and_N\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselect_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (P_i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (N_i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mcal_P_and_N\u001b[0;34m(select_id, category, data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcal_P_and_N\u001b[39m(select_id, category, data):\n\u001b[0;32m----> 2\u001b[0m     data_test \u001b[38;5;241m=\u001b[39m data[(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m select_id) \u001b[38;5;241m|\u001b[39m (\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mright_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mselect_id\u001b[49m)]\n\u001b[1;32m      4\u001b[0m     data_test_group \u001b[38;5;241m=\u001b[39m data_test\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Check if the category exists in the grouped data\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6240\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   6242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 6243\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/ops/array_ops.py:287\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 287\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/ops/array_ops.py:75\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_Q_for_image(img_id):\n",
    "    results = []\n",
    "    for c in category:\n",
    "        Q = cal_Q(img_id, c, data)\n",
    "        results.append([img_id, c, Q])\n",
    "    return results\n",
    "\n",
    "Q_ls = []\n",
    "# futures = [calculate_Q_for_image(img_id) for img_id in tqdm(img_id_ls)]\n",
    "for img_id in tqdm(img_id_ls):\n",
    "    results = calculate_Q_for_image(img_id)\n",
    "    Q_ls.extend(results)\n",
    "    if len(Q_ls) % 100 == 0:\n",
    "        Q_df = pd.DataFrame(Q_ls, columns=['Image_ID', 'Category', 'Q_Value'])\n",
    "        Q_df.to_csv(\"/data_nas/cehou/LLM_safety/image_perception.csv\", index=False)\n",
    "        print(\"Save to csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
