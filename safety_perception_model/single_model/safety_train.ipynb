{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "sys.path.append(\"/code/LLM-crime/single_model\")\n",
    "from my_models import TransformerRegressionModel, ResNet50Model, ViTClassifier\n",
    "from LLM_feature_extractor import LLaVaFeatureExtractor\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from safety_perception_dataset import *\n",
    "import neptune\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.metrics import r2_score\n",
    "import shutil\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_type</th>\n",
       "      <th>placepulse_datapath</th>\n",
       "      <th>safety_model_save_path</th>\n",
       "      <th>safety_model_save_name</th>\n",
       "      <th>parameters[\"num_epochs\"]</th>\n",
       "      <th>visual_feature_extractor</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>input_dim</th>\n",
       "      <th>adaptor_output_dim</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>lr</th>\n",
       "      <th>LLM_loaded</th>\n",
       "      <th>LLM_feature_process</th>\n",
       "      <th>loss_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>classification</td>\n",
       "      <td>/data2/cehou/LLM_safety/PlacePulse2.0/image_pe...</td>\n",
       "      <td>/data2/cehou/LLM_safety/LLM_models/safety_perc...</td>\n",
       "      <td>model_baseline.pt</td>\n",
       "      <td>50</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>mean_dim1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_type                                placepulse_datapath  \\\n",
       "0  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "1  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "2  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "3  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "4  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "5  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "6  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "7  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "8  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "9  classification  /data2/cehou/LLM_safety/PlacePulse2.0/image_pe...   \n",
       "\n",
       "                              safety_model_save_path safety_model_save_name  \\\n",
       "0  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "1  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "2  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "3  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "4  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "5  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "6  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "7  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "8  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "9  /data2/cehou/LLM_safety/LLM_models/safety_perc...      model_baseline.pt   \n",
       "\n",
       "   parameters[\"num_epochs\"] visual_feature_extractor  batch_size  input_dim  \\\n",
       "0                        50                 resnet18         128        512   \n",
       "1                        50                 resnet18         128        512   \n",
       "2                        50                 resnet18         128        512   \n",
       "3                        50                 resnet18         128        512   \n",
       "4                        50                 resnet18         128        512   \n",
       "5                        50                 resnet18         128        512   \n",
       "6                        50                 resnet18         128        512   \n",
       "7                        50                 resnet18         128        512   \n",
       "8                        50                 resnet18         128        512   \n",
       "9                        50                 resnet18         128        512   \n",
       "\n",
       "   adaptor_output_dim  num_classes     lr  LLM_loaded LLM_feature_process  \\\n",
       "0                 256            2  0.001        True           mean_dim1   \n",
       "1                 256            2  0.001        True           mean_dim1   \n",
       "2                 256            2  0.001        True           mean_dim1   \n",
       "3                 256            2  0.001        True           mean_dim1   \n",
       "4                 256            2  0.001        True           mean_dim1   \n",
       "5                 256            2  0.001        True           mean_dim1   \n",
       "6                 256            2  0.001        True           mean_dim1   \n",
       "7                 256            2  0.001        True           mean_dim1   \n",
       "8                 256            2  0.001        True           mean_dim1   \n",
       "9                 256            2  0.001        True           mean_dim1   \n",
       "\n",
       "   loss_1  \n",
       "0       1  \n",
       "1       2  \n",
       "2       1  \n",
       "3       2  \n",
       "4       1  \n",
       "5       2  \n",
       "6       1  \n",
       "7       2  \n",
       "8       1  \n",
       "9       2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "        'train_type': \"classification\",\n",
    "        'placepulse_datapath': \"/data2/cehou/LLM_safety/PlacePulse2.0/image_perception_score.csv\",\n",
    "        'safety_model_save_path' : f\"/data2/cehou/LLM_safety/LLM_models/safety_perception_model/only_img/\",\n",
    "        'safety_model_save_name':\"model_baseline.pt\",\n",
    "        \n",
    "        # model training parameters\n",
    "        'parameters[\"num_epochs\"]': 50,\n",
    "        'visual_feature_extractor': 'resnet18',\n",
    "        'batch_size': 128,\n",
    "        'input_dim': 512,\n",
    "        'adaptor_output_dim': 256,\n",
    "        'num_classes': 2,\n",
    "        'lr': 0.001,\n",
    "        'LLM_loaded': True,\n",
    "        'LLM_feature_process': 'mean_dim1',\n",
    "        'loss_1':[1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
    "\n",
    "                \n",
    "    }\n",
    "pd.DataFrame(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_paras = {\n",
    "    'debug':False,\n",
    "    'dataset_path':\"/data2/cehou/LLM_safety/img_text_data/dataset_baseline_baseline_baseline_baseline_1401.pkl\",\n",
    "    'save_model_path':\"/data2/cehou/LLM_safety/LLM_models/clip_model/test\",\n",
    "    'save_model_name':\"model_baseline_test.pt\",\n",
    "    'device':torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'batch_size':192,\n",
    "    'num_workers':4,\n",
    "    'head_lr':1e-3,\n",
    "    'image_encoder_lr':1e-4,\n",
    "    'text_encoder_lr':1e-5,\n",
    "    'weight_decay':1e-3,\n",
    "    'img_type':'PlacePulse',\n",
    "    'patience':1,\n",
    "    'factor':0.8,\n",
    "    'epochs':400,\n",
    "    'image_embedding':2048,\n",
    "    'text_embedding':768,\n",
    "    'max_length':512,\n",
    "    'size':(224,224),\n",
    "    \n",
    "    # models for image and text\n",
    "    'model_name':'resnet50',\n",
    "    'text_encoder_model':\"distilbert-base-uncased\",\n",
    "    'text_tokenizer': \"distilbert-base-uncased\",\n",
    "    'pretrained':True,\n",
    "    'trainable':True,\n",
    "    \n",
    "    # deep learning model parameters\n",
    "    'temperature':0.07,\n",
    "    'projection_dim':256,\n",
    "    'dropout':0.1,\n",
    "    'early_stopping_threshold':5,\n",
    "    \n",
    "    # safety perception\n",
    "    # 'CLIP_model_path': \"/data2/cehou/LLM_safety/LLM_models/clip_model/test/model_baseline_best.pt\",\n",
    "    'variables_save_paths': f\"/data2/cehou/LLM_safety/middle_variables/test\",\n",
    "    'safety_model_save_path' : f\"/data2/cehou/LLM_safety/LLM_models/safety_perception_model/only_img/\",\n",
    "    'placepulse_datapath': \"/data2/cehou/LLM_safety/PlacePulse2.0/image_perception_score.csv\",\n",
    "    'eval_path': \"/data2/cehou/LLM_safety/eval/test/only_img/\",\n",
    "    'train_type': 'classification',\n",
    "    'safety_epochs': 200,\n",
    "    'class_num': 2,\n",
    "    'CNN_lr': 1*1e-7,\n",
    "    'weight_on': False\n",
    "    }\n",
    "\n",
    "# run = neptune.init_run(\n",
    "#     project=\"ce-hou/Safety\",\n",
    "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmYzFmZTZkYy1iZmY3LTQ1NzUtYTRlNi1iYTgzNjRmNGQyOGUifQ==\",\n",
    "# )  # your credentials\n",
    "\n",
    "# data = pd.read_csv(cfg_paras['placepulse_datapath'])\n",
    "# data_ls = data[data['label'] != 0]\n",
    "# data_ls.loc[data_ls[data_ls['label'] == -1].index, 'label'] = 0\n",
    "# transform = get_transforms(cfg_paras['size'])\n",
    "# split_num = int(len(data_ls) * 0.8)\n",
    "\n",
    "# train_dataset = SafetyPerceptionDataset(data_ls[:split_num], transform=transform, paras=cfg_paras)\n",
    "# valid_dataset = SafetyPerceptionDataset(data_ls[split_num:], transform=transform, paras=cfg_paras)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cfg_paras['batch_size'], shuffle=True)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cfg_paras['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 创建模型实例\n",
    "\n",
    "class LLMImageFeaturePrextractor(nn.Module):\n",
    "    def __init__(self, process='mean'):\n",
    "        super(LLMImageFeaturePrextractor, self).__init__()\n",
    "        self.llava_extractor = LLaVaFeatureExtractor()\n",
    "        self.conv_dim1 = nn.Conv2d(3, 1, kernel_size=1)  # 输入3通道，输出1通道\n",
    "        self.conv_dim2 = nn.Conv2d(3, 3, kernel_size=1)  # 输入3通道，输出3通道\n",
    "        self.process = process\n",
    "    \n",
    "    def forward(self, x):\n",
    "        img_feature = self.llava_extractor.image_extractor(x)\n",
    "        \n",
    "        if self.process == 'mean_dim1':\n",
    "            img_feature = img_feature.mean(dim=(1))\n",
    "        if self.process == 'mean_dim2':\n",
    "            img_feature = img_feature.mean(dim=(2))\n",
    "        if self.process == 'max_dim1':\n",
    "            img_feature = img_feature.max(dim=(1))[0]\n",
    "        if self.process == 'max_dim2':\n",
    "            img_feature = img_feature.max(dim=(2))[0]\n",
    "        if self.process == 'reshape':\n",
    "            img_feature = img_feature.reshape(-1, img_feature.shape[3], img_feature.shape[4])\n",
    "        if self.process == 'conv_dim1':\n",
    "            img_feature = self.conv_dim1(img_feature)\n",
    "        if self.process == 'conv_dim2':\n",
    "            img_feature = self.conv_dim2(img_feature)\n",
    "        return img_feature\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, pretrained_model='resnet18'):\n",
    "        super(Extractor, self).__init__()\n",
    "        if pretrained_model == 'ViT':\n",
    "            pass\n",
    "        if pretrained_model == 'resnet50':\n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "            # 去掉最后的全连接层\n",
    "            self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        if pretrained_model == 'resnet18':\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "            # 去掉最后的全连接层\n",
    "            self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入图像 x，返回提取的特征\n",
    "        with torch.no_grad():  # 禁用梯度计算\n",
    "            features = self.model(x)\n",
    "        # 返回特征的展平（flatten）形式\n",
    "        return features.view(features.size(0), -1)\n",
    "    \n",
    "class Adaptor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        projection_dim,\n",
    "        data_type\n",
    "    ):\n",
    "        super(Adaptor, self).__init__()\n",
    "        if data_type == 'image':\n",
    "            self.projection = nn.Linear(input_dim, projection_dim)\n",
    "        elif data_type == 'text':\n",
    "            self.projection = nn.Linear(input_dim, projection_dim)\n",
    "        # self.projection = nn.Linear(cfg_paras['embedding_dim'], cfg_paras['projection_dim'])\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        # 一个简单的全连接层作为分类器\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入适配后的特征向量，输出分类结果\n",
    "        return self.fc(x)\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, extractor, adaptor, classifier):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.extractor = extractor\n",
    "        self.adaptor = adaptor\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先通过extractor提取特征，再通过adaptor处理，最后分类\n",
    "        features = self.extractor(x)\n",
    "        # print(\"extracted feature: \", features.shape)\n",
    "        adapted_features = self.adaptor(features)\n",
    "        # print(\"adapted feature: \", adapted_features.shape)\n",
    "        output = self.classifier(adapted_features)\n",
    "        # print(\"final feature\", output.shape)\n",
    "        return output\n",
    "    \n",
    "# Early Stopping类\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=20, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping triggered\")    \n",
    "\n",
    "def train(model, pbar, criterion, optimizer):\n",
    "    model.train()  # 切换到训练模式\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        # 将数据和目标移到GPU（如果有的话）\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()  # 清零梯度\n",
    "        output = model(data)  # 获取模型输出\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # target_one_hot = F.one_hot(target, num_classes=2).float()\n",
    "        # loss = criterion(output, target_one_hot)\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def eval(model, valid_loader, criterion):\n",
    "    model.eval()  # 切换到评估模式\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # 关闭梯度计算，节省内存\n",
    "        for data, target in valid_loader:\n",
    "            data, target = data.cuda(), target.cuda().long()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/50:   0%|          | 0/42 [00:03<?, ?batch/s]\n",
      "Epoch 1/50: 100%|██████████| 42/42 [00:14<00:00,  2.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 0.8317, Validation Loss: 0.6785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:  33%|███▎      | 14/42 [00:04<00:09,  2.82batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     78\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, mininterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, valid_loader, criterion)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# 输出当前epoch的训练和验证损失\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 118\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, pbar, criterion, optimizer)\u001b[0m\n\u001b[1;32m    116\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# 切换到训练模式\u001b[39;00m\n\u001b[1;32m    117\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# 将数据和目标移到GPU（如果有的话）\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcuda(), target\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    121\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 清零梯度\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 128\n",
    "input_dim = 512  # ResNet18 输出的特征维度\n",
    "adaptor_output_dim = 256  # 适配器输出的维度\n",
    "num_classes = 2  # 假设有 10 类\n",
    "LLM_loaded = True\n",
    "if LLM_loaded == False:\n",
    "    LLM_pre_extractor = LLMImageFeaturePrextractor(process='mean_dim1') # LLM将图像提取为一个浅的图像特征，维度为[3,336,336]\n",
    "    LLM_loaded = True\n",
    "\n",
    "data = pd.read_csv(cfg_paras['placepulse_datapath'])\n",
    "data_ls = data[data['label'] != 0]\n",
    "data_ls.loc[data_ls[data_ls['label'] == -1].index, 'label'] = 0\n",
    "transform = get_transforms((224,224))\n",
    "train_num = int(len(data_ls) * 0.6)\n",
    "valid_num = int(len(data_ls) * 0.2)\n",
    "\n",
    "def get_transforms(resize_size):\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((resize_size[0], resize_size[1])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )    \n",
    "\n",
    "train_dataset = SafetyPerceptionDataset(data_ls[:train_num], transform=transform, paras=cfg_paras)\n",
    "valid_dataset = SafetyPerceptionDataset(data_ls[train_num:train_num+valid_num], transform=transform, paras=cfg_paras)\n",
    "test_dataset = SafetyPerceptionDataset(data_ls[train_num+valid_num:], transform=transform, paras=cfg_paras)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "extractor = Extractor(pretrained_model='resnet18') # [128, 512]\n",
    "adaptor = Adaptor(input_dim=input_dim, projection_dim=adaptor_output_dim, data_type='image') # [128, 256]\n",
    "classifier = Classifier(input_dim=adaptor_output_dim, num_classes=num_classes) # [128, 2]\n",
    "model = FullModel(extractor, adaptor, classifier).cuda()\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCELoss()  # 二分类交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                    \n",
    "                   \n",
    "early_stopping = EarlyStopping(patience=20, verbose=True)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\",unit=\"batch\", mininterval=2.0)\n",
    "    running_loss = train(model, pbar, criterion, optimizer)\n",
    "    val_loss = eval(model, valid_loader, criterion)\n",
    "\n",
    "    # 输出当前epoch的训练和验证损失\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {running_loss / len(train_loader):.4f}, Validation Loss: {val_loss / len(valid_loader):.4f}\")\n",
    "\n",
    "    # 触发早停机制\n",
    "    early_stopping(val_loss / len(valid_loader))\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用来调试的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  测试代码\n",
    "if __name__ == '__main__':\n",
    "    # 假设输入的图像大小是 (batch_size, 3, 224, 224)，即 RGB 图像\n",
    "    batch_size = 8\n",
    "    input_dim = 512  # ResNet18 输出的特征维度\n",
    "    adaptor_output_dim = 256  # 适配器输出的维度\n",
    "    num_classes = 10  # 假设有 10 类\n",
    "    LLM_loaded = True\n",
    "    if LLM_loaded == False:\n",
    "        LLM_pre_extractor = LLMImageFeaturePrextractor(process='mean_dim1') # LLM将图像提取为一个浅的图像特征，维度为[3,336,336]\n",
    "        LLM_loaded = True\n",
    "        \n",
    "    # 从valid_loader中取出一个batch的数据\n",
    "    data_iter = iter(valid_loader)\n",
    "    images, labels = next(data_iter)\n",
    "        \n",
    "    x = LLM_pre_extractor([images[i] for i in range(8)])\n",
    "    print(x.shape)\n",
    "    \n",
    "    # 初始化模块\n",
    "    extractor = Extractor(pretrained_model='resnet18')\n",
    "    adaptor = Adaptor(input_dim=input_dim, projection_dim=adaptor_output_dim, data_type='image')\n",
    "    classifier = Classifier(input_dim=adaptor_output_dim, num_classes=num_classes)\n",
    "\n",
    "    # 组合成一个完整的神经网络\n",
    "    model = FullModel(extractor, adaptor, classifier)\n",
    "\n",
    "    # 将模型移动到与输入相同的设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    x = x.to(device)\n",
    "\n",
    "    # 前向传播\n",
    "    output = model(x)\n",
    "    print(output.shape)  # 输出分类结果的形状\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像检查\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LLM_pre_extractor([images[i] for i in range(8)])\n",
    "sns.heatmap(x[0,0].detach().cpu().numpy(), cmap='viridis', cbar=False)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_0.001_LLM_feature_process_mean_dim1\n",
      "lr_0.001_LLM_feature_process_mean_dim2\n",
      "lr_0.001_LLM_feature_process_mean\n",
      "lr_0.0001_LLM_feature_process_mean_dim1\n",
      "lr_0.0001_LLM_feature_process_mean_dim2\n",
      "lr_0.0001_LLM_feature_process_mean\n",
      "lr_1e-05_LLM_feature_process_mean_dim1\n",
      "lr_1e-05_LLM_feature_process_mean_dim2\n",
      "lr_1e-05_LLM_feature_process_mean\n",
      "lr_1e-06_LLM_feature_process_mean_dim1\n",
      "lr_1e-06_LLM_feature_process_mean_dim2\n",
      "lr_1e-06_LLM_feature_process_mean\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "variables_dict = {'lr':[0.001, 0.0001, 0.00001, 1e-6], \n",
    "                   'LLM_feature_process':['mean_dim1', 'mean_dim2', 'mean']}\n",
    "\n",
    "combinations = list(product(*variables_dict.values()))\n",
    "\n",
    "for i,j in combinations:\n",
    "    subfolder_name = f\"{list(variables_dict.keys())[0]}_{i}_{list(variables_dict.keys())[1]}_{j}\"\n",
    "    print(subfolder_name)\n",
    "# for combination in combinations:\n",
    "#     subfolder_name = ''\n",
    "#     for i, key in enumerate(variables_dict.keys()):\n",
    "#         parameters[key] = combination[i]\n",
    "#         subfolder_name += f\"{key}_{combination[i]}_\"\n",
    "#     print(subfolder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.001, 'LLM_feature_process': 'mean_dim1', 'subfolder_name': 'lr_0.001_LLM_feature_process_mean_dim1'}\n",
      "{'lr': 0.001, 'LLM_feature_process': 'mean_dim2', 'subfolder_name': 'lr_0.001_LLM_feature_process_mean_dim2'}\n",
      "{'lr': 0.001, 'LLM_feature_process': 'mean', 'subfolder_name': 'lr_0.001_LLM_feature_process_mean'}\n",
      "{'lr': 0.0001, 'LLM_feature_process': 'mean_dim1', 'subfolder_name': 'lr_0.0001_LLM_feature_process_mean_dim1'}\n",
      "{'lr': 0.0001, 'LLM_feature_process': 'mean_dim2', 'subfolder_name': 'lr_0.0001_LLM_feature_process_mean_dim2'}\n",
      "{'lr': 0.0001, 'LLM_feature_process': 'mean', 'subfolder_name': 'lr_0.0001_LLM_feature_process_mean'}\n",
      "{'lr': 1e-05, 'LLM_feature_process': 'mean_dim1', 'subfolder_name': 'lr_1e-05_LLM_feature_process_mean_dim1'}\n",
      "{'lr': 1e-05, 'LLM_feature_process': 'mean_dim2', 'subfolder_name': 'lr_1e-05_LLM_feature_process_mean_dim2'}\n",
      "{'lr': 1e-05, 'LLM_feature_process': 'mean', 'subfolder_name': 'lr_1e-05_LLM_feature_process_mean'}\n",
      "{'lr': 1e-06, 'LLM_feature_process': 'mean_dim1', 'subfolder_name': 'lr_1e-06_LLM_feature_process_mean_dim1'}\n",
      "{'lr': 1e-06, 'LLM_feature_process': 'mean_dim2', 'subfolder_name': 'lr_1e-06_LLM_feature_process_mean_dim2'}\n",
      "{'lr': 1e-06, 'LLM_feature_process': 'mean', 'subfolder_name': 'lr_1e-06_LLM_feature_process_mean'}\n"
     ]
    }
   ],
   "source": [
    "for combination in combinations:\n",
    "    input_dict = dict(zip(variables_dict.keys(), combination))\n",
    "    input_dict['subfolder_name'] = '_'.join([f\"{key}_{value}\" for key, value in input_dict.items()])\n",
    "    print(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1e-06\n",
      "LLM_feature_process: mean\n",
      "subfolder_name: lr_1e-06_LLM_feature_process_mean\n",
      "lr: 1e-06\n",
      "LLM_feature_process: mean\n",
      "subfolder_name: lr_1e-06_LLM_feature_process_mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, value in input_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, valid_loader, paras):\n",
    "    print(f'device: {paras[\"device\"]}')\n",
    "    if paras['train_type'] == 'regression':\n",
    "        input_dim = 3 * paras['size'][0] * paras['size'][1]\n",
    "        model_dim = 512\n",
    "        num_heads = 8  \n",
    "        num_layers = 6\n",
    "        dropout = paras['dropout']\n",
    "        output_dim = 1\n",
    "        model = ResNet50Model(output_dim).to(paras['device'])\n",
    "        # print(model)\n",
    "        # model = TransformerRegressionModel(input_dim, model_dim, num_heads, num_layers, output_dim, dropout).to(paras['device'])\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=paras[\"CNN_lr\"])\n",
    "        \n",
    "    elif paras['train_type'] == 'classification':\n",
    "        input_dim = 3 * paras['size'][0] * paras['size'][1]\n",
    "        output_dim = paras['class_num']\n",
    "        # model = ResNet50Model(output_dim).to(paras['device'])\n",
    "        # model = ViTClassifier(output_dim).to(paras['device'])\n",
    "        # print(model)\n",
    "        # model = ViTClassifier(num_classes=paras['class_num'],input_dim=input_dim).to(paras['device'])\n",
    "        if paras['weight_on']:\n",
    "            class_weights = torch.FloatTensor(paras['class_weights']).to(paras['device'])\n",
    "            # print(\"class_weights: \", class_weights)\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=paras[\"CNN_lr\"])\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = paras['safety_epochs']\n",
    "    best_loss = float('inf')\n",
    "    count_after_best = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_running_loss = 0.0\n",
    "        tqdm_loader = tqdm(train_loader, total=len(train_loader))\n",
    "        for inputs,labels in tqdm_loader:\n",
    "            inputs = inputs.to(paras['device']) #16, 3, 300, 400\n",
    "            # print(labels)\n",
    "            if paras['train_type'] == 'classification':\n",
    "                labels = labels.to(paras['device']).long()\n",
    "            elif paras['train_type'] == 'regression':\n",
    "                labels = labels.to(paras['device']).float()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # 16, 6\n",
    "            if outputs.shape[1] == 1:\n",
    "                outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_running_loss += loss.item()\n",
    "            # print(\"train_running_loss: \", loss.item())\n",
    "            \n",
    "            # Update tqdm description with current loss\n",
    "            tqdm_loader.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tqdm_loader.set_postfix(loss=train_running_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs,labels) in enumerate(valid_loader):\n",
    "                if paras['train_type'] == 'classification':\n",
    "                    inputs = inputs.to(paras['device'])\n",
    "                    labels = labels.to(paras['device']).long()\n",
    "                    # print(labels)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    # Record predictions and true labels\n",
    "                    if i == 0:\n",
    "                        all_preds = predicted.cpu().numpy()\n",
    "                        all_labels = labels.cpu().numpy()\n",
    "                    else:\n",
    "                        all_preds = np.concatenate((all_preds, predicted.cpu().numpy()))\n",
    "                        all_labels = np.concatenate((all_labels, labels.cpu().numpy()))\n",
    "                        \n",
    "                elif paras['train_type'] == 'regression':\n",
    "                    inputs = inputs.to(paras['device'])\n",
    "                    labels = labels.to(paras['device']).float()\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = outputs.squeeze(1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "                    if i == 0:\n",
    "                        all_preds = outputs.cpu().numpy()\n",
    "                        all_labels = labels.cpu().numpy()\n",
    "                    else:\n",
    "                        all_preds = np.concatenate((all_preds, outputs.cpu().numpy()))\n",
    "                        all_labels = np.concatenate((all_labels, labels.cpu().numpy()))\n",
    "\n",
    "        count_after_best += 1\n",
    "        if val_running_loss < best_loss:\n",
    "            best_loss = val_running_loss\n",
    "            count_after_best = 0\n",
    "            if not os.path.exists(paras['safety_model_save_path']):\n",
    "                os.makedirs(paras['safety_model_save_path'])\n",
    "            torch.save(model.state_dict(), os.path.join(paras['safety_model_save_path'], f\"best_{paras['train_type']}_model.pth\"))\n",
    "            print(f\"save the best model to {os.path.join(paras['safety_model_save_path'])}.\")\n",
    "        run[\"train/total_loss\"].append(train_running_loss/len(train_loader))\n",
    "        run[\"valid/total_loss\"].append(val_running_loss/len(valid_loader))\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_running_loss/train_loader.batch_size:.4f}, Validation Loss: {val_running_loss/valid_loader.batch_size:.4f}\")\n",
    "        if paras['train_type'] == 'classification':\n",
    "            run[\"valid/accuracy\"].append(correct / total)\n",
    "            print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "            # Calculate confusion matrix\n",
    "            cm = confusion_matrix(all_labels, all_preds)\n",
    "            # Plot confusion matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            # plt.xlabel(\"Predicted\")\n",
    "            # plt.ylabel(\"True\")\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "                        annot_kws={\"size\": 12, \"weight\": \"bold\", \"color\": \"red\"} \n",
    "                        )\n",
    "            plt.title(f\"Confusion Matrix epoch {epoch+1} acc: {correct/total:0.2%}\")\n",
    "            cm_savepath = os.path.join(paras['eval_path'], 'valid_cm')\n",
    "            if not os.path.exists(cm_savepath):\n",
    "                os.makedirs(cm_savepath)\n",
    "            plt.savefig(os.path.join(cm_savepath, f\"confusion_matrix_epoch_{epoch+1}.png\"))\n",
    "            plt.close()\n",
    "        elif paras['train_type'] == 'regression':\n",
    "            r2 = r2_score(all_preds, all_labels)\n",
    "            run[\"valid/r2_score\"].append(r2)\n",
    "            print(f\"R2 score: {r2:.2f}\")       \n",
    "            # Plot R2 score curve\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.regplot(x=all_labels, y=all_preds, scatter_kws={'s':10}, line_kws={\"color\":\"red\"})\n",
    "            plt.xlim(-0.5,1.5)\n",
    "            plt.ylim(-0.5,1.5)\n",
    "            plt.xlabel(\"True Labels\")\n",
    "            plt.ylabel(\"Predicted Labels\")\n",
    "            plt.title(f\"Regression Results epoch {epoch+1} R2: {r2:.2f}\")\n",
    "            regplot_savepath = os.path.join(paras['eval_path'], 'regression_plots')\n",
    "            if not os.path.exists(regplot_savepath):\n",
    "                os.makedirs(regplot_savepath)\n",
    "            plt.savefig(os.path.join(regplot_savepath, f\"regression_plot_epoch_{epoch+1}.png\"))\n",
    "            plt.close()\n",
    "                \n",
    "        if count_after_best > paras['early_stopping_threshold']:\n",
    "            print(\"Early Stopping!\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
