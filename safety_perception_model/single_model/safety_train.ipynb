{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "sys.path.append(\"/code/LLM-crime/single_model\")\n",
    "from my_models import TransformerRegressionModel, ResNet50Model, ViTClassifier\n",
    "from LLM_feature_extractor import LLaVaFeatureExtractor\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from safety_perception_dataset import *\n",
    "import neptune\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.metrics import r2_score\n",
    "import shutil\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_paras = {\n",
    "    'debug':False,\n",
    "    'dataset_path':\"/data2/cehou/LLM_safety/img_text_data/dataset_baseline_baseline_baseline_baseline_1401.pkl\",\n",
    "    'save_model_path':\"/data2/cehou/LLM_safety/LLM_models/clip_model/test\",\n",
    "    'save_model_name':\"model_baseline_test.pt\",\n",
    "    'device':torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'batch_size':192,\n",
    "    'num_workers':4,\n",
    "    'head_lr':1e-3,\n",
    "    'image_encoder_lr':1e-4,\n",
    "    'text_encoder_lr':1e-5,\n",
    "    'weight_decay':1e-3,\n",
    "    'img_type':'PlacePulse',\n",
    "    'patience':1,\n",
    "    'factor':0.8,\n",
    "    'epochs':400,\n",
    "    'image_embedding':2048,\n",
    "    'text_embedding':768,\n",
    "    'max_length':512,\n",
    "    'size':(224,224),\n",
    "    \n",
    "    # models for image and text\n",
    "    'model_name':'resnet50',\n",
    "    'text_encoder_model':\"distilbert-base-uncased\",\n",
    "    'text_tokenizer': \"distilbert-base-uncased\",\n",
    "    'pretrained':True,\n",
    "    'trainable':True,\n",
    "    \n",
    "    # deep learning model parameters\n",
    "    'temperature':0.07,\n",
    "    'projection_dim':256,\n",
    "    'dropout':0.1,\n",
    "    'early_stopping_threshold':5,\n",
    "    \n",
    "    # safety perception\n",
    "    # 'CLIP_model_path': \"/data2/cehou/LLM_safety/LLM_models/clip_model/test/model_baseline_best.pt\",\n",
    "    'variables_save_paths': f\"/data2/cehou/LLM_safety/middle_variables/test\",\n",
    "    'safety_model_save_path' : f\"/data2/cehou/LLM_safety/LLM_models/safety_perception_model/only_img/\",\n",
    "    'placepulse_datapath': \"/data2/cehou/LLM_safety/PlacePulse2.0/image_perception_score.csv\",\n",
    "    'eval_path': \"/data2/cehou/LLM_safety/eval/test/only_img/\",\n",
    "    'train_type': 'classification',\n",
    "    'safety_epochs': 200,\n",
    "    'class_num': 2,\n",
    "    'CNN_lr': 1*1e-7,\n",
    "    'weight_on': False\n",
    "    }\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"ce-hou/Safety\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmYzFmZTZkYy1iZmY3LTQ1NzUtYTRlNi1iYTgzNjRmNGQyOGUifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "data = pd.read_csv(cfg_paras['placepulse_datapath'])\n",
    "data_ls = data[data['label'] != 0]\n",
    "data_ls.loc[data_ls[data_ls['label'] == -1].index, 'label'] = 0\n",
    "transform = get_transforms(cfg_paras['size'])\n",
    "split_num = int(len(data_ls) * 0.8)\n",
    "\n",
    "train_dataset = SafetyPerceptionDataset(data_ls[:split_num], transform=transform, paras=cfg_paras)\n",
    "valid_dataset = SafetyPerceptionDataset(data_ls[split_num:], transform=transform, paras=cfg_paras)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cfg_paras['batch_size'], shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cfg_paras['batch_size'])\n",
    "\n",
    "model = LLaVaFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg_paras,\n",
    "        data_type\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if data_type == 'image':\n",
    "            self.projection = nn.Linear(cfg_paras['image_embedding'], cfg_paras['projection_dim'])\n",
    "        elif data_type == 'text':\n",
    "            self.projection = nn.Linear(cfg_paras['text_embedding'], cfg_paras['projection_dim'])\n",
    "        # self.projection = nn.Linear(cfg_paras['embedding_dim'], cfg_paras['projection_dim'])\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(cfg_paras['projection_dim'], cfg_paras['projection_dim'])\n",
    "        self.dropout = nn.Dropout(cfg_paras['dropout'])\n",
    "        self.layer_norm = nn.LayerNorm(cfg_paras['projection_dim'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从valid_loader中取出一个batch的数据\n",
    "data_iter = iter(valid_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# 取出前五张图像\n",
    "five_images = images[:5]\n",
    "five_labels = labels[:5]\n",
    "\n",
    "img_feature = model.image_extractor([images[i].permute(1, 2, 0) for i in range(5)]) # [batch_size, 3, 3, 336, 336]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 mean\n",
    "img_feature_mean = img_feature[0].mean(dim=(0, 1))\n",
    "img_feature_max = img_feature[0].max(dim=(0))[0].max(dim=(0))[0]\n",
    "img_feature_reshaped = img_feature[0].reshape(-1, img_feature.shape[3], img_feature.shape[4]) # [3, 3, 336, 336] -> [9, 336, 336]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg_paras,\n",
    "        data_type\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if data_type == 'image':\n",
    "            self.projection = nn.Linear(cfg_paras['image_embedding'], cfg_paras['projection_dim'])\n",
    "        elif data_type == 'text':\n",
    "            self.projection = nn.Linear(cfg_paras['text_embedding'], cfg_paras['projection_dim'])\n",
    "        # self.projection = nn.Linear(cfg_paras['embedding_dim'], cfg_paras['projection_dim'])\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(cfg_paras['projection_dim'], cfg_paras['projection_dim'])\n",
    "        self.dropout = nn.Dropout(cfg_paras['dropout'])\n",
    "        self.layer_norm = nn.LayerNorm(cfg_paras['projection_dim'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(in_features=32 * 56 * 56, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg_paras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 创建模型实例\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m feature_extractor_model \u001b[38;5;241m=\u001b[39m FeatureExtractor(\u001b[43mcfg_paras\u001b[49m, process\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(cfg_paras[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 测试模型\u001b[39;00m\n\u001b[1;32m     29\u001b[0m img_feature \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mimage_extractor([images[i]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg_paras' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, cfg_paras, process='mean'):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.llava_extractor = LLaVaFeatureExtractor()\n",
    "        self.cnn_block = SimpleCNN(num_classes=cfg_paras['class_num'])\n",
    "        self.projection_head = ProjectionHead(cfg_paras, data_type='image')\n",
    "        self.fc = nn.Linear(cfg_paras['projection_dim'], 2048)\n",
    "        self.process = process\n",
    "    \n",
    "    def forward(self, x):\n",
    "        img_feature = self.llava_extractor.image_extractor(x)\n",
    "        if self.process == 'mean':\n",
    "            img_feature = img_feature.mean(dim=(0, 1))\n",
    "        elif self.process == 'max':\n",
    "            img_feature = img_feature.max(dim=(0))[0].max(dim=(0))[0]\n",
    "        elif self.process == 'reshape':\n",
    "            img_feature = img_feature.reshape(-1, img_feature.shape[3], img_feature.shape[4])\n",
    "        img_feature = self.cnn_block(img_feature)\n",
    "        projected_feature = self.projection_head(img_feature)\n",
    "        output = self.fc(projected_feature)\n",
    "        return output\n",
    "\n",
    "# 创建模型实例\n",
    "feature_extractor_model = FeatureExtractor(cfg_paras, process='mean').to(cfg_paras['device'])\n",
    "\n",
    "# 测试模型\n",
    "img_feature = model.image_extractor([images[i].permute(1, 2, 0) for i in range(5)])\n",
    "output = feature_extractor_model(img_feature_mean)\n",
    "\n",
    "print(output.shape)  # 应该输出 torch.Size([5, 2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, criterion, optimizer, device, save_path, num_epochs=25, early_stopping_threshold=5):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.save_path = save_path\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stopping_threshold = early_stopping_threshold\n",
    "\n",
    "    def train(self):\n",
    "        best_loss = float('inf')\n",
    "        count_after_best = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            train_running_loss = 0.0\n",
    "            for inputs, labels in self.train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_running_loss += loss.item()\n",
    "\n",
    "            valid_running_loss = self.validate()\n",
    "\n",
    "            if valid_running_loss < best_loss:\n",
    "                best_loss = valid_running_loss\n",
    "                count_after_best = 0\n",
    "                torch.save(self.model.state_dict(), self.save_path)\n",
    "                print(f\"Saved best model to {self.save_path}\")\n",
    "            else:\n",
    "                count_after_best += 1\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{self.num_epochs}], Train Loss: {train_running_loss/len(self.train_loader):.4f}, Validation Loss: {valid_running_loss/len(self.valid_loader):.4f}\")\n",
    "\n",
    "            if count_after_best > self.early_stopping_threshold:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        valid_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.valid_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                valid_running_loss += loss.item()\n",
    "        return valid_running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = LLaVaFeatureExtractor(),\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.Adam(lr=cfg_paras[\"CNN_lr\"]),\n",
    "    device='cuda:1',\n",
    "    save_path=os.path.join(cfg_paras['safety_model_save_path'], \"best_llm_model.pth\"),\n",
    "    num_epochs=cfg_paras['safety_epochs'],\n",
    "    early_stopping_threshold=cfg_paras['early_stopping_threshold']\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, valid_loader, paras):\n",
    "    print(f'device: {paras[\"device\"]}')\n",
    "    if paras['train_type'] == 'regression':\n",
    "        input_dim = 3 * paras['size'][0] * paras['size'][1]\n",
    "        model_dim = 512\n",
    "        num_heads = 8  \n",
    "        num_layers = 6\n",
    "        dropout = paras['dropout']\n",
    "        output_dim = 1\n",
    "        model = ResNet50Model(output_dim).to(paras['device'])\n",
    "        # print(model)\n",
    "        # model = TransformerRegressionModel(input_dim, model_dim, num_heads, num_layers, output_dim, dropout).to(paras['device'])\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=paras[\"CNN_lr\"])\n",
    "        \n",
    "    elif paras['train_type'] == 'classification':\n",
    "        input_dim = 3 * paras['size'][0] * paras['size'][1]\n",
    "        output_dim = paras['class_num']\n",
    "        # model = ResNet50Model(output_dim).to(paras['device'])\n",
    "        # model = ViTClassifier(output_dim).to(paras['device'])\n",
    "        # print(model)\n",
    "        # model = ViTClassifier(num_classes=paras['class_num'],input_dim=input_dim).to(paras['device'])\n",
    "        if paras['weight_on']:\n",
    "            class_weights = torch.FloatTensor(paras['class_weights']).to(paras['device'])\n",
    "            # print(\"class_weights: \", class_weights)\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=paras[\"CNN_lr\"])\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = paras['safety_epochs']\n",
    "    best_loss = float('inf')\n",
    "    count_after_best = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_running_loss = 0.0\n",
    "        tqdm_loader = tqdm(train_loader, total=len(train_loader))\n",
    "        for inputs,labels in tqdm_loader:\n",
    "            inputs = inputs.to(paras['device']) #16, 3, 300, 400\n",
    "            # print(labels)\n",
    "            if paras['train_type'] == 'classification':\n",
    "                labels = labels.to(paras['device']).long()\n",
    "            elif paras['train_type'] == 'regression':\n",
    "                labels = labels.to(paras['device']).float()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # 16, 6\n",
    "            if outputs.shape[1] == 1:\n",
    "                outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_running_loss += loss.item()\n",
    "            # print(\"train_running_loss: \", loss.item())\n",
    "            \n",
    "            # Update tqdm description with current loss\n",
    "            tqdm_loader.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tqdm_loader.set_postfix(loss=train_running_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs,labels) in enumerate(valid_loader):\n",
    "                if paras['train_type'] == 'classification':\n",
    "                    inputs = inputs.to(paras['device'])\n",
    "                    labels = labels.to(paras['device']).long()\n",
    "                    # print(labels)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    # Record predictions and true labels\n",
    "                    if i == 0:\n",
    "                        all_preds = predicted.cpu().numpy()\n",
    "                        all_labels = labels.cpu().numpy()\n",
    "                    else:\n",
    "                        all_preds = np.concatenate((all_preds, predicted.cpu().numpy()))\n",
    "                        all_labels = np.concatenate((all_labels, labels.cpu().numpy()))\n",
    "                        \n",
    "                elif paras['train_type'] == 'regression':\n",
    "                    inputs = inputs.to(paras['device'])\n",
    "                    labels = labels.to(paras['device']).float()\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = outputs.squeeze(1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "                    if i == 0:\n",
    "                        all_preds = outputs.cpu().numpy()\n",
    "                        all_labels = labels.cpu().numpy()\n",
    "                    else:\n",
    "                        all_preds = np.concatenate((all_preds, outputs.cpu().numpy()))\n",
    "                        all_labels = np.concatenate((all_labels, labels.cpu().numpy()))\n",
    "\n",
    "        count_after_best += 1\n",
    "        if val_running_loss < best_loss:\n",
    "            best_loss = val_running_loss\n",
    "            count_after_best = 0\n",
    "            if not os.path.exists(paras['safety_model_save_path']):\n",
    "                os.makedirs(paras['safety_model_save_path'])\n",
    "            torch.save(model.state_dict(), os.path.join(paras['safety_model_save_path'], f\"best_{paras['train_type']}_model.pth\"))\n",
    "            print(f\"save the best model to {os.path.join(paras['safety_model_save_path'])}.\")\n",
    "        run[\"train/total_loss\"].append(train_running_loss/len(train_loader))\n",
    "        run[\"valid/total_loss\"].append(val_running_loss/len(valid_loader))\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_running_loss/train_loader.batch_size:.4f}, Validation Loss: {val_running_loss/valid_loader.batch_size:.4f}\")\n",
    "        if paras['train_type'] == 'classification':\n",
    "            run[\"valid/accuracy\"].append(correct / total)\n",
    "            print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "            # Calculate confusion matrix\n",
    "            cm = confusion_matrix(all_labels, all_preds)\n",
    "            # Plot confusion matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            # plt.xlabel(\"Predicted\")\n",
    "            # plt.ylabel(\"True\")\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "                        annot_kws={\"size\": 12, \"weight\": \"bold\", \"color\": \"red\"} \n",
    "                        )\n",
    "            plt.title(f\"Confusion Matrix epoch {epoch+1} acc: {correct/total:0.2%}\")\n",
    "            cm_savepath = os.path.join(paras['eval_path'], 'valid_cm')\n",
    "            if not os.path.exists(cm_savepath):\n",
    "                os.makedirs(cm_savepath)\n",
    "            plt.savefig(os.path.join(cm_savepath, f\"confusion_matrix_epoch_{epoch+1}.png\"))\n",
    "            plt.close()\n",
    "        elif paras['train_type'] == 'regression':\n",
    "            r2 = r2_score(all_preds, all_labels)\n",
    "            run[\"valid/r2_score\"].append(r2)\n",
    "            print(f\"R2 score: {r2:.2f}\")       \n",
    "            # Plot R2 score curve\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.regplot(x=all_labels, y=all_preds, scatter_kws={'s':10}, line_kws={\"color\":\"red\"})\n",
    "            plt.xlim(-0.5,1.5)\n",
    "            plt.ylim(-0.5,1.5)\n",
    "            plt.xlabel(\"True Labels\")\n",
    "            plt.ylabel(\"Predicted Labels\")\n",
    "            plt.title(f\"Regression Results epoch {epoch+1} R2: {r2:.2f}\")\n",
    "            regplot_savepath = os.path.join(paras['eval_path'], 'regression_plots')\n",
    "            if not os.path.exists(regplot_savepath):\n",
    "                os.makedirs(regplot_savepath)\n",
    "            plt.savefig(os.path.join(regplot_savepath, f\"regression_plot_epoch_{epoch+1}.png\"))\n",
    "            plt.close()\n",
    "                \n",
    "        if count_after_best > paras['early_stopping_threshold']:\n",
    "            print(\"Early Stopping!\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
